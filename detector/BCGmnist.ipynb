{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab28bfb-7879-4c63-8975-08e3ea8b4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/CGAN/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/envs/CGAN/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2410037-f4f5-4846-bf9f-f94df0da1428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]]\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# get ID dataset\n",
    "root = './../../data/mnist'  \n",
    "\n",
    "trainset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "testset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = datasets.MNIST(root, train=True, download=True, transform=trainset)\n",
    "test_set = datasets.MNIST(root, train=False, download=True, transform=testset)\n",
    "\n",
    "batch_size = 3000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "X_sub, _ = next(data_iter)\n",
    "X_sub = X_sub.numpy()\n",
    "print(X_sub)\n",
    "print(len(X_sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2f9539-2842-4ce4-a3da-93af74aeafb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]\n",
      "\n",
      "\n",
      " [[[-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   ...\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]\n",
      "   [-0.42421296 -0.42421296 -0.42421296 ... -0.42421296 -0.42421296\n",
      "    -0.42421296]]]]\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# get OOD KnockoffNet dataset \n",
    "root = './../../data/emnist' \n",
    "# 定义数据集的变换\n",
    "trainset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "testset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = datasets.EMNIST(root, split='letters', train=True, download=True, transform=trainset)\n",
    "test_set = datasets.EMNIST(root, split='letters', train=False, download=True, transform=testset)\n",
    "batch_size = 3000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "data_iter1 = iter(train_loader)\n",
    "X_sub1, _ = next(data_iter1)\n",
    "X_sub1 = X_sub1.numpy()\n",
    "print(X_sub1)\n",
    "print(len(X_sub1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d41bef08-93b4-4807-93e1-4e29233d2905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concat_data = np.concatenate((X_sub, X_sub1), axis=0)\n",
    "#print(concat_data)\n",
    "print(len(concat_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03b9b8c-183d-4224-a5ae-15147fb2676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据长度: 3000\n",
      "原始数据形状: (3000, 1, 28, 28)\n",
      "后1500个数据的长度: 1500\n",
      "后1500个数据的形状: (1500, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get OOD JBDA dataset \n",
    "loaded_data = np.load('../../mnist/X_sub1500.npy')\n",
    "print(\"原始数据长度:\", len(loaded_data))\n",
    "print(\"原始数据形状:\", loaded_data.shape)\n",
    "\n",
    "# 获取后1500个数据\n",
    "last_1500_data = loaded_data[-1500:]\n",
    "print(\"后1500个数据的长度:\", len(last_1500_data))\n",
    "print(\"后1500个数据的形状:\", last_1500_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06fbf75b-582d-480d-9b36-8dd09f359a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n"
     ]
    }
   ],
   "source": [
    "# 连接数组\n",
    "concat_data = np.concatenate((concat_data, last_1500_data), axis=0)\n",
    "#print(concat_data)\n",
    "print(len(concat_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68c39cfe-4d1d-4233-8790-e93054122f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "(7500,)\n"
     ]
    }
   ],
   "source": [
    "# 创建全零标签数组\n",
    "labels_sub = np.zeros(concat_data.shape[0])\n",
    "# 将concat_data分成三个部分，并为每个部分赋予相应的标签\n",
    "labels_sub[:3000] = 0  # 前1500个元素的标签为0\n",
    "labels_sub[3000:6000] = 1  # 接下来的1500个元素的标签为1\n",
    "labels_sub[6000:] = 2  # 最后1500个元素的标签为2\n",
    "#labels_sub[4500:] = 3\n",
    "\n",
    "# 打印标签数组的长度和0形状\n",
    "print(len(labels_sub))\n",
    "print(labels_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e35e392-746d-4662-9821-60412c5af004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4115,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.2242, -0.2242, -0.2242,  ..., -0.6242, -0.6242, -0.4242],\n",
      "          [-0.2242, -0.2242, -0.6242,  ..., -0.6242, -0.2242, -0.4242],\n",
      "          [-0.6242, -0.2242, -0.2242,  ..., -0.2242, -0.2242, -0.4242],\n",
      "          ...,\n",
      "          [-0.6242, -0.6242, -0.2242,  ..., -0.6242, -0.6242, -0.6242],\n",
      "          [-0.2242, -0.2242, -0.6242,  ..., -0.6242, -0.2242, -0.6242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.6242, -0.6242, -0.6242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.6242, -0.6242, -0.2242,  ..., -0.6242, -0.6242, -0.4242],\n",
      "          [-0.2242, -0.6242, -0.6242,  ..., -0.6242, -0.6242, -0.4242],\n",
      "          [-0.2242, -0.2242, -0.2242,  ..., -0.6242, -0.2242, -0.4242],\n",
      "          ...,\n",
      "          [-0.2242, -0.2242, -0.6242,  ..., -0.2242, -0.2242, -0.4242],\n",
      "          [-0.6242, -0.6242, -0.6242,  ..., -0.6242, -0.6242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]) tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.6242, -0.6242, -0.6242,  ..., -0.2242, -0.2242, -0.4242],\n",
      "          [-0.2242, -0.6242, -0.6242,  ..., -0.2242, -0.2242, -0.4242],\n",
      "          [-0.6242, -0.6242, -0.2242,  ..., -0.2242, -0.2242, -0.4242],\n",
      "          ...,\n",
      "          [-0.6242, -0.6242, -0.6242,  ..., -0.6242, -0.2242, -0.4242],\n",
      "          [-0.2242, -0.2242, -0.6242,  ..., -0.2242, -0.6242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.2242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
      "\n",
      "\n",
      "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          ...,\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]) tensor([1, 1, 2,  ..., 1, 0, 2]) tensor([0, 2, 0,  ..., 0, 1, 0])\n",
      "训练集长度: 6000\n",
      "测试集长度: 1500\n",
      "训练标签长度: 6000\n",
      "测试标签长度: 1500\n",
      "torch.Size([6000, 1, 28, 28])\n",
      "torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 转换为PyTorch的Tensor对象\n",
    "X_tensor = torch.from_numpy(concat_data).float()\n",
    "y_tensor = torch.from_numpy(labels_sub).long()\n",
    "\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "print(X_train, X_test, y_train, y_test)\n",
    "print(\"训练集长度:\", len(X_train))\n",
    "print(\"测试集长度:\", len(X_test))\n",
    "print(\"训练标签长度:\", len(y_train))\n",
    "print(\"测试标签长度:\", len(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89b24d77-5540-4348-ba60-e38070459c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassifier,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128,128, 3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, 3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(128, 128, 1,padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv8 = nn.Conv2d(128, 256, 3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(256, 256, 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(512, 512, 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        self.fc14 = nn.Linear(512*4*4,1024)\n",
    "        self.drop1 = nn.Dropout2d()\n",
    "        self.fc15 = nn.Linear(1024,1024)\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.fc16 = nn.Linear(1024,3)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        # print(\" x shape \",x.size())\n",
    "        x = x.view(-1,512*4*4)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc16(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9950bef-43f2-4d30-b2a5-9db97b317502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义逻辑回归模型\n",
    "num_classes = 3\n",
    "#model = MultiClassifier()\n",
    "model = MultiClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# 将数据和标签送入设备（如GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "# 创建训练数据加载器\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# 创建测试数据加载器\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b50768-abc4-445e-ad1e-4ff75f6396e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设您的模型已经定义为 model\n",
    "# 假设您的数据加载器已经定义为 train_loader\n",
    "# 假设您的设备已经定义为 device\n",
    "\n",
    "num_epochs = 55\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam 优化器和 L2 正则化\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# 交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 学习率调度器\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # 每100个批次打印一次\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "# 保存训练好的模型参数\n",
    "\n",
    "torch.save(model.state_dict(), './mnist/attackmnist3.pth')\n",
    "\n",
    "print('训练完成')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cfb32d7-5908-4c8a-806c-cfd3ffe598db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9953\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估模型性能\n",
    "model.load_state_dict(torch.load('./mnist/attackmnist3.pth'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        outputs = model(inputs)\n",
    "#       print(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "#        print(predicted)\n",
    "#        print(labels)\n",
    "        if predicted == labels:\n",
    "            correct += 1\n",
    "    test_accuracy = correct / len(test_dataset)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3173764-d461-499b-8656-04b982b47f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR: 0.0069\n",
      "FNR: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# 初始化计数器\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "model.load_state_dict(torch.load('./mnist/attackmnist3.pth'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "# 在测试集上评估模型性能\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)    \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # 更新计数器\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            if label == 0 and prediction == 0:\n",
    "                TP += 1\n",
    "            elif label != 0 and prediction == 0:\n",
    "                FP += 1\n",
    "            elif label == 0 and prediction != 0:\n",
    "                FN += 1\n",
    "            elif label != 0 and prediction != 0:\n",
    "                TN += 1\n",
    "\n",
    "# 计算FPR和FNR\n",
    "FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "FNR = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "\n",
    "print(f\"FPR: {FPR:.4f}\")\n",
    "print(f\"FNR: {FNR:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b196d849-bf8f-42da-9ce9-e5aadbf95e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNR for class 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "TP = 0  # 真阳性\n",
    "FN = 0  # 假阴性\n",
    "model.load_state_dict(torch.load('./mnist/attackmnist3.pth'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            if label == 2 and prediction == 2:\n",
    "                TP += 1  # 真阳性：正确预测为类别1\n",
    "            elif label == 2 and prediction != 2:\n",
    "                FN += 1  # 假阴性：错误预测为非类别1\n",
    "\n",
    "# 计算 FNR\n",
    "FNR = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "print(f\"FNR for class 2: {FNR:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec00a9dc-2a64-4fff-b7a6-b7556dcd38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|████████████████████████████████████| 10000/10000 [00:17<00:00, 577.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 10000\n",
      "IN: 9978\n",
      "Knockoff Data: 22\n",
      "JBDA Data: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = './../../data/mnist'  # 数据集存储的根目录\n",
    "\n",
    "# 定义数据集的变换\n",
    "trainset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "testset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_set = datasets.MNIST(root, train=True, download=True, transform=trainset)\n",
    "test_set = datasets.MNIST(root, train=False, download=True, transform=testset)\n",
    "\n",
    "# 定义 DataLoader\n",
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "IN_count = 0\n",
    "JBDA_count = 0\n",
    "Knockoff_count = 0\n",
    "input_count = 0\n",
    "\n",
    "# 修改模型的全连接层\n",
    "num_classes = 3  # 新的类别数量\n",
    "model = MultiClassifier()\n",
    "model.load_state_dict(torch.load('./mnist/attackmnist3.pth'))\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# 在测试集上评估模型性能\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in tqdm(test_loader, desc=\"Validation Progress\"):\n",
    "        input_count += 1\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "#        print(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        if predicted == 0 :\n",
    "            IN_count += 1\n",
    "        elif predicted == 1 :\n",
    "            Knockoff_count += 1\n",
    "        elif predicted == 2:\n",
    "            JBDA_count += 1\n",
    "    print(f\"input: {input_count:}\")\n",
    "    print(f\"IN: {IN_count:}\")\n",
    "    print(f\"Knockoff Data: {Knockoff_count:}\")\n",
    "    print(f\"JBDA Data: {JBDA_count:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4de11338-6d7e-414c-89c7-022cce470bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|████████████████████████████████████| 20800/20800 [00:36<00:00, 571.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 20800\n",
      "IN: 125\n",
      "Knockoff Data: 20673\n",
      "JBDA Data: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = './../../data/emnist'  # 数据集存储的根目录\n",
    "# 定义数据集的变换\n",
    "trainset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "testset = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_set = datasets.EMNIST(root, split='letters', train=True, download=True, transform=trainset)\n",
    "test_set = datasets.EMNIST(root, split='letters', train=False, download=True, transform=testset)\n",
    "batch_size = 3000\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "# 定义 DataLoader\n",
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "IN_count = 0\n",
    "JBDA_count = 0\n",
    "Knockoff_count = 0\n",
    "input_count = 0\n",
    "\n",
    "# 修改模型的全连接层\n",
    "num_classes = 3  # 新的类别数量\n",
    "model = MultiClassifier()\n",
    "model.load_state_dict(torch.load('./mnist/attackmnist3.pth'))\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# 在测试集上评估模型性能\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in tqdm(test_loader, desc=\"Validation Progress\"):\n",
    "        input_count += 1\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "#        print(outputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        if predicted == 0 :\n",
    "            IN_count += 1\n",
    "        elif predicted == 1 :\n",
    "            Knockoff_count += 1\n",
    "        elif predicted == 2:\n",
    "            JBDA_count += 1\n",
    "    print(f\"input: {input_count:}\")\n",
    "    print(f\"IN: {IN_count:}\")\n",
    "    print(f\"Knockoff Data: {Knockoff_count:}\")\n",
    "    print(f\"JBDA Data: {JBDA_count:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189db2c-d8b1-49c7-bf3b-efdaed0b4ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
